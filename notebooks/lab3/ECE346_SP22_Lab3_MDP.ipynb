{"cells":[{"cell_type":"markdown","metadata":{"id":"htT-8pxNJb7T"},"source":["# Passenger pick up MDP\n","In this part you will attempt to formulate the MDP of our Minicity to solve the passenger pick up problem using Value Iteration and Policy Iteration."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"sJxurl6iCc08"},"outputs":[],"source":["# @markdown Run this cell to install dependencies.\n","%%capture\n","\n","% cd /content\n","! git clone https://github.com/buzi-princeton/MDP.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"czsa9bXaCl42"},"outputs":[],"source":["from MDP.mdp import MDP\n","from MDP.visualizer.minicity import MinicityVisualizer\n","import numpy as np\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"fmd_MJy4IwzY"},"outputs":[],"source":["# @markdown Run this cell to test our dependencies.\n","# TEST\n","# Run the following test to see if your MDP code fetch is working\n","print(\"Test MDP class\")\n","a = [1, 2, 3, 4]\n","b = [5, 6, 7, 8]\n","c = [23, 24, 25]\n","\n","mdp = MDP(states=[a, b, c], actions=[1,2,3,4])\n","\n","for i in range(4*4*3):\n","  state = mdp.get_state(i)\n","  real_state = mdp.get_real_state_value(i)\n","  index = mdp.get_index(real_state)\n","  if i != index:\n","    raise ValueError(\"Something is wrong\")\n","  if i%10==0:\n","    print(i, state, real_state, index)\n","\n","print(\"Everything is correct!\")\n","print(\"\\nTest Minicity Visualizer\")\n","\n","import os\n","\n","folder = \"figure\"\n","sub_folder = \"minicity\"\n","\n","fig_folder = os.path.join(\"/content\", folder)\n","fig_prog_folder = os.path.join(fig_folder, sub_folder)\n","os.makedirs(fig_prog_folder, exist_ok=True)\n","\n","visualizer = MinicityVisualizer(fig_prog_folder=fig_prog_folder)\n","visualizer.reset(current_pos=0, goal=6)\n","visualizer.plot()\n","for i in range(1, 7):\n","    visualizer.update_pos(i)\n","\n","import imageio\n","from IPython.display import Image\n","from tqdm.notebook import tqdm\n","\n","gif_path = os.path.join(fig_prog_folder, 'result.gif')\n","length = len([i for i in os.listdir(os.path.join(fig_prog_folder)) if \".png\" in i])\n","\n","with imageio.get_writer(gif_path, mode='I') as writer:\n","  for i in tqdm(range(length)):\n","    print(i, end='\\r')\n","    filename = os.path.join(fig_prog_folder, str(i)+\".png\")\n","    image = imageio.imread(filename)\n","    writer.append_data(image)\n","Image(open(gif_path,'rb').read(), width=400)"]},{"cell_type":"markdown","metadata":{"id":"zme0yUfFKfzD"},"source":["## MDP formulation of Minicity\n","\n","Below is the sample 2-state MDP as discussed in lab 3 handout. Try to run the below code and play around to understand how MDP class works."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i2oognBAKiPd"},"outputs":[],"source":["class TwoStateMDP(MDP):\n","  def __init__(self):\n","    self.states = [\"s1\", \"s2\"]\n","    self.actions = [\"a0\", \"a1\"]\n","    self.gam = 0.9\n","    \n","    # call the parent class\n","    # notice that the state is a list of state variables\n","    super().__init__(\n","      states=[self.states], actions=self.actions)\n","    self.populate_data()\n","    \n","  def populate_data(self):\n","    # add all routes from s1\n","    self.add_route([\"s1\"],\"a0\",[\"s1\"])\n","    self.add_route([\"s1\"],\"a1\",[\"s2\"])\n","    # add all routes from s2\n","    self.add_route([\"s2\"],\"a0\",[\"s2\"])\n","    self.add_route([\"s2\"],\"a1\",[\"s2\"])\n","    \n","    # let's populate the reward, assuming r>0 is 0.5\n","    for a in self.a:\n","      self.add_reward([\"s1\"],a,0.5)\n","      self.add_reward([\"s2\"],a,1.5)\n","\n","twoStateMDP = TwoStateMDP()\n","print(twoStateMDP.get_index([\"s1\"]))\n","print(twoStateMDP.get_state(0))\n","print(twoStateMDP.get_real_state_value(0))"]},{"cell_type":"markdown","metadata":{"id":"kpiGg8KRK6Q7"},"source":["Now let's try to build our Minicity MDP.\n","We have 1 positional state variable $p_{cur}$, 1 directional state variable $d$ and 1 goal state variable $p_{goal}$.\n","$s = \\{p_{cur}, d,  p_{goal}\\}$\n","\n","$p_{cur} \\in \\{0â€¦6\\},  p_{goal} \\in \\{3, 4, 5, 6\\}$,  and \n","$d \\in \\{cw, ccw\\}$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qBhX6vqYK2fS"},"outputs":[],"source":["class Minicity(MDP):\n","  def __init__(self):\n","    self.positional_states = [0, 1, 2, 3, 4, 5, 6]\n","    self.goal_states = [3, 4, 5, 6]\n","    self.directional_states = [\"cw\", \"ccw\"]\n","    self.actions = [\"forward\", \"left\", \"right\", \"switch\"]\n","    self.gam = 0.9\n","\n","    super().__init__(states=[self.positional_states, self.directional_states, self.goal_states], actions=self.actions)\n","    \n","    self.populate_data()\n","  \n","  def populate_data(self):\n","    # populate state transition function and reward function\n","    ####\n","    def add_route_and_reward(state_current, action, state_new, reward):\n","      reward_pickup = 10 if state_new[0] == state_current[2] else 0\n","      self.add_route(state_current, action, state_new)\n","      self.add_reward(state_current, action, reward + reward_pickup)\n","    \n","    for gs in self.goal_states: # transitions independent of goal state\n","      # inner loop\n","      for dir, ndir in [(\"cw\", \"ccw\"), (\"ccw\", \"cw\")]: # transitions independent of orientation\n","        # position 0\n","        add_route_and_reward([0, dir, gs], \"forward\", [1, dir,  gs], -1)\n","        add_route_and_reward([0, dir, gs], \"left\",    [0, dir,  gs], -10) # crash\n","        add_route_and_reward([0, dir, gs], \"right\",   [4, dir,  gs], -1)\n","        add_route_and_reward([0, dir, gs], \"switch\",  [0, ndir, gs], -1)\n","        # position 1\n","        add_route_and_reward([1, dir, gs], \"forward\", [2, dir,  gs], -1)\n","        add_route_and_reward([1, dir, gs], \"left\",    [1, dir,  gs], -10) # crash\n","        add_route_and_reward([1, dir, gs], \"right\",   [5, dir,  gs], -1)\n","        add_route_and_reward([1, dir, gs], \"switch\",  [1, ndir, gs], -1)\n","        # position 2\n","        add_route_and_reward([2, dir, gs], \"forward\", [0, dir,  gs], -1)\n","        add_route_and_reward([2, dir, gs], \"left\",    [2, dir,  gs], -10) # crash\n","        add_route_and_reward([2, dir, gs], \"right\",   [3, dir,  gs], -1)\n","        add_route_and_reward([2, dir, gs], \"switch\",  [2, ndir, gs], -1)\n","      \n","      # outer loop\n","      # position 3\n","      add_route_and_reward([3, \"cw\",  gs], \"forward\", [6, \"cw\",  gs], -1)\n","      add_route_and_reward([3, \"cw\",  gs], \"left\",    [3, \"cw\",  gs], -10) # crash\n","      add_route_and_reward([3, \"cw\",  gs], \"right\",   [2, \"cw\",  gs], -1)\n","      add_route_and_reward([3, \"cw\",  gs], \"switch\",  [3, \"ccw\", gs], -1)\n","      add_route_and_reward([3, \"ccw\", gs], \"forward\", [4, \"ccw\", gs], -1)\n","      add_route_and_reward([3, \"ccw\", gs], \"left\",    [2, \"ccw\", gs], -1)\n","      add_route_and_reward([3, \"ccw\", gs], \"right\",   [3, \"ccw\", gs], -10) # crash\n","      add_route_and_reward([3, \"ccw\", gs], \"switch\",  [3, \"cw\",  gs], -1)\n","      # position 4\n","      add_route_and_reward([4, \"cw\",  gs], \"forward\", [3, \"cw\",  gs], -1)\n","      add_route_and_reward([4, \"cw\",  gs], \"left\",    [4, \"cw\",  gs], -10) # crash\n","      add_route_and_reward([4, \"cw\",  gs], \"right\",   [0, \"cw\",  gs], -1)\n","      add_route_and_reward([4, \"cw\",  gs], \"switch\",  [4, \"ccw\", gs], -1)\n","      add_route_and_reward([4, \"ccw\", gs], \"forward\", [5, \"ccw\", gs], -1)\n","      add_route_and_reward([4, \"ccw\", gs], \"left\",    [0, \"ccw\", gs], -1)\n","      add_route_and_reward([4, \"ccw\", gs], \"right\",   [4, \"ccw\", gs], -10) # crash\n","      add_route_and_reward([4, \"ccw\", gs], \"switch\",  [4, \"cw\",  gs], -1)\n","      # position 5\n","      add_route_and_reward([5, \"cw\",  gs], \"forward\", [4, \"cw\",  gs], -1)\n","      add_route_and_reward([5, \"cw\",  gs], \"left\",    [5, \"cw\",  gs], -10) # crash\n","      add_route_and_reward([5, \"cw\",  gs], \"right\",   [1, \"cw\",  gs], -1)\n","      add_route_and_reward([5, \"cw\",  gs], \"switch\",  [5, \"ccw\", gs], -1)\n","      add_route_and_reward([5, \"ccw\", gs], \"forward\", [6, \"ccw\", gs], -1)\n","      add_route_and_reward([5, \"ccw\", gs], \"left\",    [1, \"ccw\", gs], -1)\n","      add_route_and_reward([5, \"ccw\", gs], \"right\",   [5, \"ccw\", gs], -10) # crash\n","      add_route_and_reward([5, \"ccw\", gs], \"switch\",  [5, \"cw\",  gs], -1)\n","      # position 6\n","      add_route_and_reward([6, \"cw\",  gs], \"forward\", [5, \"cw\",  gs], -1)\n","      add_route_and_reward([6, \"cw\",  gs], \"left\",    [6, \"cw\",  gs], -10) # crash\n","      add_route_and_reward([6, \"cw\",  gs], \"right\",   [6, \"cw\",  gs], -10) # crash\n","      add_route_and_reward([6, \"cw\",  gs], \"switch\",  [6, \"cw\",  gs], -10) # keep dir\n","      add_route_and_reward([6, \"ccw\", gs], \"forward\", [3, \"ccw\", gs], -1)\n","      add_route_and_reward([6, \"ccw\", gs], \"left\",    [6, \"ccw\", gs], -10) # crash\n","      add_route_and_reward([6, \"ccw\", gs], \"right\",   [6, \"ccw\", gs], -10) # crash\n","      add_route_and_reward([6, \"ccw\", gs], \"switch\",  [6, \"ccw\", gs], -10) # keep dir\n","    ####"]},{"cell_type":"markdown","metadata":{"id":"6lBDhHfZLult"},"source":["## Value and Policy Iteration\n","Let's now write the value iteration and policy iteration method"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LRSKM1uyLzTq"},"outputs":[],"source":["def value_iteration(threshold = .001, mdp=None):\n","  if mdp is None:\n","    raise ValueError(\"MDP cannot be None\")\n","  numa, nums, R, P = mdp.get_mdp()\n","  V_star = np.zeros(nums)\n","  pi_star = np.zeros(nums)\n","  \n","  ####\n","  count = 0\n","  while True:\n","    V_old = V_star\n","    \n","    # compute quality matrix with Q.shape = [num_states, num_actions]\n","    Q = R + mdp.gam * np.einsum(\"jia,j->ia\", P, V_star) # i = current state, j = next state, a = action\n","    # find action with highest quality for each state\n","    pi_star = np.argmax(Q, axis=1)\n","    V_star = Q[range(nums), pi_star]\n","    \n","    # update iteration counter\n","    count += 1\n","    # check convergence\n","    if np.max(np.abs(V_star - V_old)) < threshold:\n","      break\n","  \n","  print(\"Value iteration: {} iterations\".format(count))\n","  ####\n","\n","  return V_star, pi_star\n","\n","def policy_eval(policy, threshold = .001, mdp=None):\n","  if mdp is None:\n","    raise ValueError(\"MDP cannot be None\")\n","  numa, nums, R, P = mdp.get_mdp()\n","  V = np.zeros(nums)\n","  \n","  ####\n","  # compute value vector by solving LSE\n","  A = np.eye(nums) - mdp.gam * P[:, range(nums), policy].T\n","  b = R[range(nums), policy]\n","  V = np.linalg.solve(A, b)\n","  ####\n","\n","  return V\n","\n","def policy_iteration(threshold = .001, mdp=None):\n","  if mdp is None:\n","    raise ValueError(\"MDP cannot be None\")\n","  numa, nums, R, P = mdp.get_mdp()\n","  # initialize a random policy with length nums and action randomly assigned from numa\n","  pi_star = np.random.randint(0, numa, nums)\n","  V_star = np.zeros(nums)\n","  \n","  ####\n","  count = 0\n","  while True:\n","    pi_old = pi_star\n","\n","    # policy evaluation\n","    V_star = policy_eval(pi_star, threshold=threshold, mdp=mdp)\n","    # policy update\n","    Q = R + mdp.gam * np.einsum(\"jia,j->ia\", P, V_star) # i = current state, j = next state, a = action\n","    pi_star = np.argmax(Q, axis=1)\n","    \n","    # update iteration counter\n","    count += 1\n","    # check convergence\n","    if (pi_star == pi_old).all():\n","      break\n","\n","  # policy evaluation of final policy\n","  V_star = policy_eval(pi_star, threshold=threshold, mdp=mdp)\n","\n","  print(\"Policy iteration: {} iterations\".format(count))\n","  ####\n","  \n","  return V_star, pi_star"]},{"cell_type":"markdown","metadata":{"id":"76tCE4P6Qh1a"},"source":["Run the below code to test your written Value iteration and Policy iteration. You should have similar optimal policy across the two methods."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cu7B-SvAPhHt"},"outputs":[],"source":["# Test policy and value iteration\n","minicity = Minicity()\n","V_star_value, pi_star_value = value_iteration(mdp=minicity)\n","print(\"Value iteration\")\n","print(\"V_star: \", V_star_value)\n","print(\"pi_star: \", pi_star_value)\n","\n","V_star_policy, pi_star_policy = policy_iteration(mdp=minicity)\n","print(\"Policy iteration\")\n","print(\"V_star: \", V_star_policy)\n","print(\"pi_star: \", pi_star_policy)\n","\n","if not np.array_equal(pi_star_value, pi_star_policy):\n","  print(\"Warning: Your pi_star between value iteration and policy iteration is different!\")\n","  print(\"Try to run these two different policies in the next test case to see if it makes sense\")"]},{"cell_type":"markdown","metadata":{"id":"ZaHus0tNQuIb"},"source":["Let's now try to use the computed policy into solving our Minicity passenger pick up MDP. Given the initial state $s=[1, cw, 6]$, what should be the sequence of actions taken, and what is the cumulative reward?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cxCyM5kDLI97"},"outputs":[],"source":["# Test pi_star\n","minicity = Minicity()\n","V_star, pi_star = value_iteration(mdp=minicity)\n","print(\"V_star: \", V_star)\n","print(\"pi_star: \", pi_star)\n","\n","# Test calculated pi_star\n","state = [1, \"cw\", 6]\n","\n","def rollout_policy(mdp, state, policy):\n","  state_index = mdp.get_index(state)\n","  action_index = policy[state_index]\n","\n","  action = mdp.a[action_index]\n","  reward = mdp.R[state_index, action_index]\n","  next_state = mdp.get_real_state_value(np.argmax(mdp.P[:, state_index, action_index]))\n","\n","  return next_state, action, reward\n","\n","while True:\n","  # get the next action from pi_star\n","  # get next state, and continue until we get to the goal\n","  # display the reward, and the actions taken so far \n","  # to solve initial state [1, \"cw\", 6]\n","  ####\n","  next_state, action, reward = rollout_policy(minicity, state, pi_star)\n","  print(\"Action: {:8}   State transition: {} -> {}   Reward: {:4}\".format(action, state, next_state, reward))\n","  state = next_state\n","  ####\n","  \n","  if state[0] == state[2]:\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A2V-x7pHM5lD"},"outputs":[],"source":["# figure folder\n","folder = \"figure\"\n","sub_folder = \"minicity\"\n","\n","fig_folder = os.path.join(\"/content\", folder)\n","fig_prog_folder = os.path.join(fig_folder, sub_folder)\n","os.makedirs(fig_prog_folder, exist_ok=True)"]},{"cell_type":"markdown","metadata":{"id":"rnveftEZRHVl"},"source":["## Checkpoint 1\n","Put everything together, let's run 5 continuous random test cases and make a beautiful GIF of our car moving in the Minicity to pick up passengers!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-1CRx-k7PM4y"},"outputs":[],"source":["import random\n","from tqdm.notebook import tqdm\n","\n","visualizer = MinicityVisualizer(fig_prog_folder=fig_prog_folder)\n","\n","for i in tqdm(range(5)):\n","  converged = False\n","  goal = random.choice([3, 4, 5, 6])\n","  pos = random.choice([0, 1, 2, 3, 4, 5, 6])\n","  \n","  while goal == pos:\n","    goal = random.choice([3, 4, 5, 6])\n","    pos = random.choice([0, 1, 2, 3, 4, 5, 6])\n","\n","  direction = random.choice([\"cw\", \"ccw\"])\n","  state = [pos, direction, goal]\n","\n","  visualizer.reset(current_pos=state[0], goal = state[2])\n","  visualizer.plot()\n","\n","  while not converged:\n","    visualizer.update_pos(state[0], dir=state[1])\n","\n","    if state[0] == state[2]:\n","      converged = True\n","\n","    ## YOUR CODE HERE\n","    state, _, _ = rollout_policy(minicity, state, pi_star)\n","    ####"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FUj2kpsARVa0"},"outputs":[],"source":["import imageio\n","from IPython.display import Image\n","from tqdm.notebook import tqdm\n","\n","gif_path = os.path.join(fig_prog_folder, 'result.gif')\n","length = len([i for i in os.listdir(os.path.join(fig_prog_folder)) if \".png\" in i])\n","\n","with imageio.get_writer(gif_path, mode='I') as writer:\n","  for i in tqdm(range(length)):\n","    print(i, end='\\r')\n","    filename = os.path.join(fig_prog_folder, str(i)+\".png\")\n","    image = imageio.imread(filename)\n","    writer.append_data(image)\n","Image(open(gif_path,'rb').read(), width=400)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"ECE346_SP22_Lab3_MDP.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
